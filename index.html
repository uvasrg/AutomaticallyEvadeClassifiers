<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <title>
    EvadeML
  </title>

  <link href="http://gmpg.org/xfn/11" rel="profile">
<meta http-equiv="content-type" content="text/html; charset=utf-8">


<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="">
<meta name="generator" content="Hugo 0.71.0" />

  <meta property="og:title" content="EvadeML" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="//evademl.org/" />


  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/base-min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/pure-min.css">
  
  
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/grids-responsive-min.css">
  
  

  <link rel="stylesheet" href="//evademl.org/css/srg.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link href='//fonts.googleapis.com/css?family=Open+Sans:400,400italic,200,100,700,300,500,600,800' rel='stylesheet' type='text/css'>
  <link href='//fonts.googleapis.com/css?family=Libre+Baskerville:400,700,400italic' rel='stylesheet' type='text/css'>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/rotunda.png">
  <link rel="shortcut icon" href="/rotunda.png">

  


  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
  </script>


    
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/tomorrow-night-bright.min.css">
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>


  

  

  
</head>

<body>
    <div id="layout" class="pure-g">
        <div class="sidebar pure-u-1 pure-u-md-1-4">
  <div class="header">
    <p class="brand-group">

<a href="https://www.cs.virginia.edu/yanjun/gQdata.htm">Maching Learning Group</a><br>
and <a href="http://www.jeffersonswheel.org">Security Research Group</a><br>
<a href="http://www.cs.virginia.edu">University of Virginia</a>
</p>



    <a href="//evademl.org"><h1 class="brand-title">EvadeML</h1></a>
    <p class="brand-tagline">Machine Learning in the Presence of Adversaries</p>



    

  </div>
</div>


        <div class="content pure-u-1 pure-u-md-3-4">
            <a name="top"></a>
            

<section id="main">
  <div>
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="is-robust-machine-learning-possible">Is Robust Machine Learning Possible?</h1>
<p>Machine learning has shown remarkable success in solving complex
classification problems, but current machine learning techniques
produce models that are vulnerable to adversaries who may wish to
confuse them, especially when used for security applications like
malware classification.</p>
<!-- raw HTML omitted -->
<p>The key assumption of machine learning is that a model that is trained
on training data will perform well in deployment because the training
data is representative of the data that will be seen when the
classifier is deployed.</p>
<p>When machine learning classifiers are used in security applications,
however, adversaries may be able to generate samples that exploit the
invalidity of this assumption.</p>
<p>Our project is focused on understanding, evaluating, and improving the
effectiveness of machine learning methods in the presence of motivated
and sophisticated adversaries.</p>
<h2 id="projects">Projects</h2>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->Hybrid Batch Attacks<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><br>
Query-efficient method to find adversarial examples for black-box victim model by combining transfer and optimization attacks, and by prioritizing easy-to-attack seeds.
<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->
Reduce search space for adversaries by coalescing inputs. <!-- raw HTML omitted -->(Top row shows $\ell_0$ adversarial examples, squeezed by median smoothing.)<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->
Method to empirically measure concentration of real datasets, finding that it does not
explain the lack of robustness of state-of-the-art models.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="papers">Papers</h2>
<p>Jack Prescott, <!-- raw HTML omitted -->Xiao Zhang<!-- raw HTML omitted -->, and David Evans. <a href="https://arxiv.org/abs/2103.12913"><em>Improved Estimation of Concentration Under ℓ<!-- raw HTML omitted -->p<!-- raw HTML omitted -->-Norm Distance Metrics Using Half Spaces</em></a>. In <a href="https://iclr.cc/Conferences/2021/"><em>Ninth International Conference on Learning Representations</em></a> (ICLR). May 2021. [<a href="https://arxiv.org/abs/2103.12913">arXiv</a>, <a href="https://openreview.net/forum?id=BUlyHkzjgmA">Open Review</a>] [<a href="https://github.com/jackbprescott/EMC_HalfSpaces">Code</a>]</p>
<p>Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian. <a href="/docs/hybridbatch.pdf"><em>Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries</em></a>. In <a href="https://www.usenix.org/conference/usenixsecurity20"><em>29<!-- raw HTML omitted -->th<!-- raw HTML omitted --> USENIX Security Symposium</em></a>. Boston, MA. August 12–14, 2020. [<a href="/docs/hybridbatch.pdf">PDF</a>] [<a href="https://arxiv.org/abs/1908.07000">arXiV</a>] [<a href="https://github.com/suyeecav/Hybrid-Attack">Code</a>]</p>
<p>Saeed Mahloujifar<!-- raw HTML omitted --><!-- raw HTML omitted -->★<!-- raw HTML omitted --><!-- raw HTML omitted -->, Xiao Zhang<!-- raw HTML omitted --><!-- raw HTML omitted -->★<!-- raw HTML omitted --><!-- raw HTML omitted -->, Mohammad Mahmoody, and David Evans. <a href="/docs/empirically-measuring-concentration.pdf"><em>Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness</em></a>. In <a href="https://nips.cc/Conferences/2019/"><em>NeurIPS 2019</em></a>. Vancouver, December 2019. (Earlier versions appeared in <a href="https://debug-ml-iclr2019.github.io/"><em>Debugging Machine Learning Models</em></a> and <a href="https://sites.google.com/view/safeml-iclr2019"><em>Safe Machine Learning: Specification, Robustness and Assurance</em></a>, workshops attached to <!-- raw HTML omitted -->Seventh International Conference on Learning Representations<!-- raw HTML omitted --> (ICLR). New Orleans. May 2019. [<a href="/docs/empirically-measuring-concentration.pdf">PDF</a>] [<a href="https://arxiv.org/abs/1905.12202">arXiv</a>] [<a href="https://jeffersonswheel.org/empirically-measuring-concentration/">Post</a>] [<a href="https://github.com/xiaozhanguva/Measure-Concentration">Code</a>]</p>
<p>Xiao Zhang and David Evans. <a href="/docs/cost-sensitive-robustness.pdf"><em>Cost-Sensitive Robustness against Adversarial Examples</em></a>. In <!-- raw HTML omitted --><!-- raw HTML omitted -->Seventh International Conference on Learning Representations<!-- raw HTML omitted --><!-- raw HTML omitted --> (ICLR). New Orleans. May 2019. [<!-- raw HTML omitted -->arXiv<!-- raw HTML omitted -->] [<!-- raw HTML omitted -->OpenReview<!-- raw HTML omitted -->] [<!-- raw HTML omitted -->PDF<!-- raw HTML omitted -->]</p>
<p>Weilin Xu, David Evans, Yanjun Qi. <a href="/docs/featuresqueezing.pdf"><em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em></a>.
<a href="https://www.ndss-symposium.org/ndss2018/"><em>2018 Network and Distributed System Security Symposium</em></a>. 18-21 February, San Diego, California. Full paper (15 pages): [<a href="/docs/featuresqueezing.pdf">PDF</a>]</p>
<p>Weilin Xu, Yanjun Qi, and David Evans. <a href="/docs/evademl.pdf"><em>Automatically Evading
Classifiers A Case Study on PDF Malware Classifiers</em></a>.  <a href="https://www.internetsociety.org/events/ndss-symposium-2016"><em>Network and Distributed Systems Symposium 2016</em></a>, 21-24 February 2016, San Diego, California. Full paper (15 pages): [<a href="/docs/evademl.pdf">PDF</a>]</p>
<p><a href="papers/">More Papers&hellip;</a></p>
<h2 id="talks">Talks</h2>
<!-- raw HTML omitted -->
<p><a href="talks/">More Talks&hellip;</a></p>
<h2 id="code">Code</h2>
<p><strong>Hybrid Batch Attacks:</strong> <a href="https://github.com/suyeecav/Hybrid-Attack">https://github.com/suyeecav/Hybrid-Attack</a></p>
<p><strong>Empirically Measuring Concentration:</strong> <a href="https://github.com/xiaozhanguva/Measure-Concentration">https://github.com/xiaozhanguva/Measure-Concentration</a></p>
<p><strong>EvadeML-Zoo:</strong> <a href="https://github.com/mzweilin/EvadeML-Zoo">https://github.com/mzweilin/EvadeML-Zoo</a></p>
<p><strong>Genetic Evasion:</strong> <a href="https://github.com/uvasrg/EvadeML">https://github.com/uvasrg/EvadeML</a> (Weilin Xu)</p>
<p><strong>Cost-Sensitive Robustness:</strong> <a href="https://github.com/xiaozhanguva/Cost-Sensitive-Robustness">https://github.com/xiaozhanguva/Cost-Sensitive-Robustness</a> (Xiao Zhang)</p>
<p><strong>Adversarial Learning Playground</strong>: <a href="https://github.com/QData/AdversarialDNN-Playground">https://github.com/QData/AdversarialDNN-Playground</a> (Andrew Norton) (mostly supersceded by the EvadeML-Zoo toolkit)</p>
<p><strong>Feature Squeezing:</strong> <a href="https://github.com/uvasrg/FeatureSqueezing">https://github.com/uvasrg/FeatureSqueezing</a> (Weilin Xu) (supersceded by the EvadeML-Zoo toolkit)</p>
<h2 id="team">Team</h2>
<p><a href="https://hannahxchen.github.io/">Hannah Chen</a> (PhD student, working on adversarial natural language processing)<br>
<a href="https://sites.google.com/site/mahmadjonas/">Mainuddin Ahmad Jonas</a> (PhD student, working on adversarial examples)<br>
<a href="https://github.com/suyeecav">Fnu Suya</a> (PhD student, working on batch attacks)<br>
<a href="https://people.virginia.edu/~xz7bc/">Xiao Zhang</a> (PhD student, working on cost-sensitive adversarial robustness)</p>
<p><a href="https://www.cs.virginia.edu/evans">David Evans</a> (Faculty Co-Advisor)<br>
<a href="https://www.cs.virginia.edu/yanjun/">Yanjun Qi</a> (Faculty Co-Advisor for Weilin Xu)<br>
<a href="https://www.ytian.info/">Yuan Tian</a> (Faculty Co-Advisor for Fnu Suya)</p>
<h3 id="alumni">Alumni</h3>
<p><a href="http://www.cs.virginia.edu/~wx4ed/">Weilin Xu</a> (PhD Student who initiated project, lead work on <a href="/squeezing">Feature Squeezing</a> and <a href="/gpevasion">Genetic Evasion</a>, now at Intel Research, Oregon)</p>
<p>Johannes Johnson (Undergraduate researcher working on malware classification, summer 2018)<br>
Anant Kharkar (Undergraduate Researcher worked on <a href="/gpevasion">Genetic Evasion</a>, 2016-2018)<br>
<a href="http://www.noahdkim.com/">Noah Kim</a> (Undergraduate Researcher worked on <a href="/zoo">EvadeML-Zoo</a>, 2017)<br>
Yuancheng Lin (Undergraduate researchers working on adversarial examples, summer 2018)<br>
Felix Park (Undergradaute Researcher, worked on color-aware preprocessors, 2017-2018)<br>
Helen Simecek (Undergraduate researcher working on <a href="/gpevasion">Genetic Evasion</a>, 2017-2019)<br>
Matthew Wallace (Undergraduate researcher working on natural language deception, 2018-2019; now at University of Wisconsin)</p>

        
    
        
    
        
    
        
    
        
    
  </div>
</section>

        <div class="footer">
	<hr class="thin" />


	<p></p>
</div>

        </div>
    </div>
    
</body>
</html>
