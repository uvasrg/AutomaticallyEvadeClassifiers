+++
title = "Papers"
+++

Xiao Zhang and David Evans. <a href="https://arxiv.org/abs/2107.03250"><em>Incorporating Label Uncertainty in Understanding Adversarial Robustness</em></a>. In <a href="https://iclr.cc/Conferences/2022">10<sup>th</sup> International Conference on Learning Representations</a> (ICLR). April 2022. [<a href="https://arxiv.org/abs/2107.03250">arXiv</a>] [<a href="https://openreview.net/forum?id=6ET9SzlgNX">OpenReview</a>] [<a href="https://github.com/xiaozhanguva/Intrinsic_robustness_label_uncertainty">Code</a>]

Yulong Tian, Fnu Suya, Fengyuan Xu and David Evans. <a href="https://ieeexplore.ieee.org/document/9737144"><em>Stealthy Backdoors as Compression Artifacts</em></a>. <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">IEEE Transactions on Information Forensics and Security</a> (Volume 17). 16 March 2022. [<a href="https://uvasrg.github.io/docs/stealthy-backdoors.pdf">PDF</a>] [<a href="https://arxiv.org/abs/2104.15129">arXiv</a>] [<a href="https://ieeexplore.ieee.org/document/9737144">IEEE Page</a>] [<a href="https://github.com/yulongtzzz/Stealthy-Backdoors-as-Compression-Artifacts">Code</a>]

Fnu Suya,  Saeed Mahloujifar,  Anshuman Suri, David Evans, and Yuan Tian. <a href="https://arxiv.org/abs/2006.16469"><em>Model-Targeted Poisoning Attacks with Provable Convergence</em></a>. In <a href="https://icml.cc/Conferences/2021">38<sup>th</sup> International Conference on Machine Learning (ICML). July 2021. [<a href="https://arxiv.org/abs/2006.16469">arXiv</a>] [<a href="https://proceedings.mlr.press/v139/suya21a.html">PMLR</a>] [<a href="http://proceedings.mlr.press/v139/suya21a/suya21a.pdf">(PDF)</a>] [<a href="https://github.com/suyeecav/model-targeted-poisoning">Code</a>] [<a href="https://uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/">Blog</a>]

Jack Prescott, <a href="https://people.virginia.edu/~xz7bc/">Xiao Zhang</a>, and David Evans. [_Improved Estimation of Concentration Under &#8467;<sub>p</sub>-Norm Distance Metrics Using Half Spaces_](https://arxiv.org/abs/2103.12913). In [_Ninth International Conference on Learning Representations_](https://iclr.cc/Conferences/2021/) (ICLR). May 2021. [[arXiv](https://arxiv.org/abs/2103.12913), [Open Review](https://openreview.net/forum?id=BUlyHkzjgmA)] [[Code](https://github.com/jackbprescott/EMC_HalfSpaces)]

Hannah Chen, Yangfeng Ji, and David Evans. <a href="https://arxiv.org/abs/2011.01856""><em>Finding Friends and Flipping Frenemies: Automatic
Paraphrase Dataset Augmentation Using Graph Theory</em></a>. In <a href="https://www.aclweb.org/anthology/volumes/2020.findings-emnlp/">Findings of ACL: Empirical Methods in Natural Language Processing</a>. 16 &ndash;18 Novemeber 2020. [<a href="https://arxiv.org/pdf/2011.01856.pdf">PDF</a>] [<a href="https://arxiv.org/abs/2011.01856">Arxiv</a>] [<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.426/">ACL</a>] [<a href="https://github.com/hannahxchen/automatic-paraphrase-dataset-augmentation">Code</a>]

Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian. [_Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries_](/docs/hybridbatch.pdf). In [_29<sup>th</sup> USENIX Security Symposium_](https://www.usenix.org/conference/usenixsecurity20). Boston, MA. August 12&ndash;14, 2020. [[PDF](/docs/hybridbatch.pdf)] [[ArXiV](https://arxiv.org/abs/1908.07000)] [[Code](https://github.com/suyeecav/Hybrid-Attack)]


Sicheng Zhu, Xiao Zhang, and David Evans. [_Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization_](https://www.cs.virginia.edu/evans/pubs/icml2020). In International Conference on Machine Learning (ICML). 12&ndash;18 July 2020. [<a href="https://arxiv.org/abs/2002.11798">arXiv</a>]

Xiao Zhang<sup><font size="-2">&#9733;</font></sup>, Jinghui
Chen<sup><font size="-2">&#9733;</font></sup>, Quanquan Gu, David
Evans. [_Understanding the Intrinsic Robustness of Image Distributions using Conditional&nbsp;Generative&nbsp;Models_](https://www.cs.virginia.edu/evans/pubs/aistats2020/). In <a href="https://aistats.org">23<sup>rd</sup> International Conference on Artificial Intelligence and Statistics</em></a> (AISTATS). Palermo, Italy. June 3&ndash;5,2020. [<a href="https://arxiv.org/pdf/2003.00378.pdf">PDF</a>] [<a href="https://arxiv.org/abs/2003.00378">arXiv</a>] [<a href="https://github.com/xiaozhanguva/Intrinsic-Rob">Code</a>]

Saeed Mahloujifar<sup><font size="-2">&#9733;</font></sup>, Xiao Zhang<sup><font size="-2">&#9733;</font></sup>, Mohammad Mahmoody, and David Evans. [_Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness_](/docs/empirically-measuring-concentration.pdf). In [_NeurIPS 2019_](https://nips.cc/Conferences/2019/). Vancouver, December 2019. (Earlier versions appeared in [_Debugging Machine Learning Models_](https://debug-ml-iclr2019.github.io/) and [_Safe Machine Learning: Specification, Robustness and Assurance_](https://sites.google.com/view/safeml-iclr2019), workshops attached to <em>Seventh International Conference on Learning Representations</em> (ICLR). New Orleans. May 2019. [[PDF](/docs/empirically-measuring-concentration.pdf)] [[arXiv](https://arxiv.org/abs/1905.12202)] [[Post](https://jeffersonswheel.org/empirically-measuring-concentration/)] [[Code](https://github.com/xiaozhanguva/Measure-Concentration)]

Xiao Zhang and David Evans. [_Cost-Sensitive Robustness against Adversarial Examples_](/docs/cost-sensitive-robustness.pdf). In <a
href="https://iclr.cc/Conferences/2019"><em>Seventh International Conference on Learning Representations</em></a> (ICLR). New Orleans. May 2019. [<a href="https://arxiv.org/abs/1810.09225">arXiv</a>] [<a
href="https://openreview.net/forum?id=BygANhA9tQ">OpenReview</a>] [<a href="/docs/cost-sensitive-robustness.pdf">PDF</a>]

Weilin Xu, David Evans, Yanjun Qi. [_Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks_](/docs/featuresqueezing.pdf). 
[_2018 Network and Distributed System Security Symposium_](https://www.ndss-symposium.org/ndss2018/). 18-21 February, San Diego, California. Full paper (15 pages): [[PDF](/docs/featuresqueezing.pdf)]

Qixue Xiao, Kang Li, Deyue Zhang, and Weilin Xu. [_Security Risks in Deep Learning Implementations_](https://arxiv.org/abs/1711.11008). <a href="https://www.ieee-security.org/TC/SPW2018/DLS/#"><em>1st Deep Learning and Security Workshop</em></a> (co-located with the 39th <em>IEEE Symposium on Security and Privacy</em>). San Francisco, California. 24 May 2018. [[PDF](https://arxiv.org/pdf/1711.11008.pdf)]

Weilin Xu, David Evans, Yanjun Qi. [_Feature Squeezing Mitigates and Detects
Carlini/Wagner Adversarial Examples_](https://arxiv.org/abs/1705.10686). arXiv preprint, 30 May 2017. [[PDF](https://arxiv.org/pdf/1705.10686.pdf), 3 pages]

Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, Yanjun Qi. [_DeepCloak: Masking Deep Neural Network Models for Robustness against Adversarial Samples_](https://arxiv.org/abs/1702.06763). ICLR Workshops, 24-26 April 2017. [[PDF](https://arxiv.org/pdf/1702.06763.pdf)]

Weilin Xu, Yanjun Qi, and David Evans. [_Automatically Evading
Classifiers A Case Study on PDF Malware Classifiers_](/docs/evademl.pdf).  [_Network and Distributed Systems Symposium 2016_](https://www.internetsociety.org/events/ndss-symposium-2016), 21-24 February 2016, San Diego, California. Full paper (15 pages): [[PDF](/docs/evademl.pdf)]

