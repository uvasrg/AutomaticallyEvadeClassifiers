<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>EvadeML</title>
    <link>//evademl.org/</link>
    <description>Recent content on EvadeML</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="//evademl.org/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Adversarial DNN Playground</title>
      <link>//evademl.org/playground/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/playground/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatically Evading Classifiers</title>
      <link>//evademl.org/gpevasion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/gpevasion/</guid>
      <description>EvadeML is an evolutionary framework based on genetic programming for automatically finding variants that evade detection by machine learning-based malware classifiers.
    Machine learning is widely used to develop classifiers for security tasks. However, the robustness of these methods against motivated adversaries is uncertain. In this work, we propose a generic method to evaluate the robustness of classifiers under attack. The key idea is to stochastically manipulate a malicious sample to find a variant that preserves the malicious behavior but is classified as benign by the classifier.</description>
    </item>
    
    <item>
      <title>Cost-Sensitive Robustness</title>
      <link>//evademl.org/costsensitive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/costsensitive/</guid>
      <description>Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. However, these methods assume that all the adversarial transformations provide equal value for adversaries, which is seldom the case in real-world applications.
We advocate for cost-sensitive robustness as the criteria for measuring the classifier&amp;rsquo;s performance for specific tasks. We encode the potential harm of different adversarial transformations in a cost matrix, and propose a general objective function to adapt the robust training method of Wong &amp;amp; Kolter (2018) to optimize for cost-sensitive robustness.</description>
    </item>
    
    <item>
      <title>Empirically Measuring Concentration</title>
      <link>//evademl.org/concentration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/concentration/</guid>
      <description>Estimating the Intrinsic Robustness for Image Benchmarks    Recent theoretical results, starting with Gilmer et al.&amp;lsquo;s Adversarial Spheres (2018), show that if inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable.c The key insight from this line of research is that concentration of measure gives lower bound on adversarial risk for a large collection of classifiers (e.g. imperfect classifiers with risk at least $\alpha$), which further implies the impossibility results for robust learning against adversarial examples.</description>
    </item>
    
    <item>
      <title>EvadeML-Zoo</title>
      <link>//evademl.org/zoo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/zoo/</guid>
      <description>EvadeML-Zoo is a benchmarking and visualization tool for Adversarial Machine Learning.
Code https://github.com/mzweilin/EvadeML-Zoo</description>
    </item>
    
    <item>
      <title>EvadeML: Evading Machine Learning Classifiers</title>
      <link>//evademl.org/main/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/main/</guid>
      <description>Is Robust Machine Learning Possible? Machine learning has shown remarkable success in solving complex classification problems, but current machine learning techniques produce models that are vulnerable to adversaries who may wish to confuse them, especially when used for security applications like malware classification.
The key assumption of machine learning is that a model that is trained on training data will perform well in deployment because the training data is representative of the data that will be seen when the classifier is deployed.</description>
    </item>
    
    <item>
      <title>Feature Squeezing</title>
      <link>//evademl.org/squeezing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/squeezing/</guid>
      <description>Detecting Adversarial Examples in Deep Neural Networks Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from expensive computation.</description>
    </item>
    
    <item>
      <title>Papers</title>
      <link>//evademl.org/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/papers/</guid>
      <description>Xiao Zhang and David Evans. Incorporating Label Uncertainty in Understanding Adversarial Robustness. In 10th International Conference on Learning Representations (ICLR). April 2022. [arXiv] [OpenReview] [Code]
Yulong Tian, Fnu Suya, Fengyuan Xu and David Evans. Stealthy Backdoors as Compression Artifacts. IEEE Transactions on Information Forensics and Security (Volume 17). 16 March 2022. [PDF] [arXiv] [IEEE Page] [Code]
Fnu Suya, Saeed Mahloujifar, Anshuman Suri, David Evans, and Yuan Tian. Model-Targeted Poisoning Attacks with Provable Convergence.</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>//evademl.org/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/talks/</guid>
      <description>Selected talks about our work on adversarial machine learning.
Trustworthy Machine Learning Mini-course at 19th International School on Foundations of Security Analysis and Design. Bertinoro, Italy. 26–28 August 2019. [Slides]
Can Machine Learning Ever Be Trustworthy? University of Maryland, Booz Allen Hamilton Distinguished Colloquium. 7 December 2018. [SpeakerDeck] [Video]
Mutually Assured Destruction and the Impending AI Apocalypse  
 Keynote talk at USENIX Workshop of Offensive Technologies, Baltimore, Maryland, 13 August 2018.</description>
    </item>
    
  </channel>
</rss>